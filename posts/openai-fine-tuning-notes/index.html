<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Openai Fine Tuning Notes | Generated Blog by Dathan capturing his random thoughts</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="OpenAI fine tuning with a notebook in VSCode AI is the future. Co-pilot is the future. Building a finetuned layer for GPT-X will enable products to produce results faster, with less upfront effort for a fraction of the cost.
Setting up a training set Let&rsquo;s assume for this post, it is based on some structured data. That structured data has been tagged by a human, and there is a lot of it."><meta name=generator content="Hugo 0.85.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/blog/ananke/css/main.min.css><meta property="og:title" content="Openai Fine Tuning Notes"><meta property="og:description" content="OpenAI fine tuning with a notebook in VSCode AI is the future. Co-pilot is the future. Building a finetuned layer for GPT-X will enable products to produce results faster, with less upfront effort for a fraction of the cost.
Setting up a training set Let&rsquo;s assume for this post, it is based on some structured data. That structured data has been tagged by a human, and there is a lot of it."><meta property="og:type" content="article"><meta property="og:url" content="https://dathan.github.io/blog/posts/openai-fine-tuning-notes/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-21T10:06:01-04:00"><meta property="article:modified_time" content="2022-11-21T10:06:01-04:00"><meta itemprop=name content="Openai Fine Tuning Notes"><meta itemprop=description content="OpenAI fine tuning with a notebook in VSCode AI is the future. Co-pilot is the future. Building a finetuned layer for GPT-X will enable products to produce results faster, with less upfront effort for a fraction of the cost.
Setting up a training set Let&rsquo;s assume for this post, it is based on some structured data. That structured data has been tagged by a human, and there is a lot of it."><meta itemprop=datePublished content="2022-11-21T10:06:01-04:00"><meta itemprop=dateModified content="2022-11-21T10:06:01-04:00"><meta itemprop=wordCount content="601"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Openai Fine Tuning Notes"><meta name=twitter:description content="OpenAI fine tuning with a notebook in VSCode AI is the future. Co-pilot is the future. Building a finetuned layer for GPT-X will enable products to produce results faster, with less upfront effort for a fraction of the cost.
Setting up a training set Let&rsquo;s assume for this post, it is based on some structured data. That structured data has been tagged by a human, and there is a lot of it."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/blog/ class="f3 fw2 hover-white no-underline white-90 dib">Generated Blog by Dathan capturing his random thoughts</a><div class="flex-l items-center"></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://dathan.github.io/blog/posts/openai-fine-tuning-notes/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://dathan.github.io/blog/posts/openai-fine-tuning-notes/&text=Openai%20Fine%20Tuning%20Notes" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://dathan.github.io/blog/posts/openai-fine-tuning-notes/&title=Openai%20Fine%20Tuning%20Notes" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Openai Fine Tuning Notes</h1><time class="f6 mv4 dib tracked" datetime=2022-11-21T10:06:01-04:00>November 21, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id=openai-fine-tuning-with-a-notebook-in-vscode>OpenAI fine tuning with a notebook in VSCode</h1><p>AI is the future. Co-pilot is the future. Building a finetuned layer for GPT-X will enable products to produce results faster, with less upfront effort for a fraction of the cost.</p><h2 id=setting-up-a-training-set>Setting up a training set</h2><p>Let&rsquo;s assume for this post, it is based on some structured data. That structured data has been tagged by a human, and there is a lot of it. Rougly 3 MB of data.</p><p>The question now is <code>how does one fine tune gpt-3 with this data</code>?</p><h2 id=jump-right-in>Jump Right in</h2><h3 id=generate-the-data>Generate the data.</h3><pre><code>import pandas as pd # data analysis package
# Two-dimensional, size-mutable, potentially heterogeneous tabular data.
df = pd.DataFrame(dataset, columns = ['prompt','completion'])
print(df.nunique())
df.head()
</code></pre><pre><code>prompt        5065
completion      87
</code></pre><p>Notice the completions. This is directly corresponding to a field in the openai client <code>classification_n_classes</code>. Note: <em>if the train and valid set do not have the completion you&rsquo;ll receive an error described below</em></p><h3 id=dump-the-data>Dump the data</h3><pre><code>df.to_json(&quot;tags_completion.jsonl&quot;, orient='records', lines=True)

</code></pre><p>This command turns the data frames of prompt and compeltion to a file called tags_completion, which we will use openai to generate a valid and train set.</p><h3 id=generate-the-valid-and-trainset-for-the-classification>Generate the valid and trainset for the classification</h3><pre><code>model = 'ada'  # can be ada, babbage or curie
n_epochs = 4
batch_size = 4
learning_rate_multiplier = 0.1
prompt_loss_weight = 0.1
classification_n_classes = 87
</code></pre><p><code>!openai api fine_tunes.create -t "tags_completion_prepared_train.jsonl" -v "tags_completion_prepared_valid.jsonl" --compute_classification_metrics --classification_n_classes $classification_n_classes --no_check_if_files_exist -m $model --n_epochs $n_epochs --batch_size $batch_size --learning_rate_multiplier $learning_rate_multiplier --prompt_loss_weight $prompt_loss_weight</code></p><h3 id=meaning-of-the-inputs>Meaning of the inputs</h3><ul><li><em>model</em> - The name of the base model to fine-tune. You can select one of &ldquo;ada&rdquo;, &ldquo;babbage&rdquo;, &ldquo;curie&rdquo;, or &ldquo;davinci&rdquo;. To learn more about these models, see the Models documentation.</li><li><em>n_epochs</em> - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.</li><li><em>batch_size</em> - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we&rsquo;ve found that larger batch sizes tend to work better for larger datasets.</li><li><em>learning_rate_multiplier</em> - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we&rsquo;ve found that larger learning rates often perform better with larger batch sizes.</li><li><em>compute_classification_metrics</em> - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.</li></ul><h3 id=encountered-problems-and-resolutions>Encountered Problems and Resolutions</h3><h3 id=problem-the-number-of-classes-in-file-njsdeo0sslpvgfsk2an0iinj-does-not-match-the-number-of-classes-specified-in-the-hyperparameters>Problem: The number of classes in file-njsdeo0sSlPVGFsK2an0iinJ does not match the number of classes specified in the hyperparameters</h3><p>What this means is that the train or valid file does not have enough data for each class in the nural network. There are recommendations to make this work.</p><h4 id=recommendations>Recommendations</h4><ul><li>The <code>completion</code> SHOULD have a 100 entries per class</li><li>The <code>completion</code> tokens MUST be distinct enough for the GPT-2 tokenizer. Basically 3/4 of the word must be distinct enough in the completion for classes.</li><li>The length of <code>prompt</code> and <code>completion</code> MUST not be larger than 2048 characters.</li></ul><h3 id=problem-if-compute_classification_metrics-is-true-each-of-the-classes-must-start-with-a-different-token>Problem: If <code>compute_classification_metrics</code> is <code>True</code>, each of the classes must start with a different token</h3><ul><li>The completion is not distinct enough based on GPT-2 tokenize specifications.</li><li><a href="https://beta.openai.com/tokenizer?view=bpe">https://beta.openai.com/tokenizer?view=bpe</a> to see the token overlap</li></ul><h2 id=progress>Progress</h2><p>Most of my time is massaging the data to fine tune gpt-3. I am very close and will update this post once I get past my last error.</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://dathan.github.io/blog/>&copy; Generated Blog by Dathan capturing his random thoughts 2022</a><div></div></div></footer></body></html>