<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Postmortem of a 2005 Flickr Outage Modernized for Today | Engineering Thoughts</title><meta name=keywords content><meta name=description content="Review of Flickr Architecture Before we delve into the Postmortem let&rsquo;s review the details of the database architecture and Flickr&rsquo;s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it&rsquo;s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years."><meta name=author content="Me"><link rel=canonical href=https://dathan.github.io/blog/posts/postmortem-of-a-2005-flickr-outage-modernized-for-today/><meta name=google-site-verification content="G-M2HJLX1HD5"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-M2HJLX1HD5"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M2HJLX1HD5",{anonymize_ip:!1})}</script><meta property="og:title" content="Postmortem of a 2005 Flickr Outage Modernized for Today"><meta property="og:description" content="Review of Flickr Architecture Before we delve into the Postmortem let&rsquo;s review the details of the database architecture and Flickr&rsquo;s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it&rsquo;s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years."><meta property="og:type" content="article"><meta property="og:url" content="https://dathan.github.io/blog/posts/postmortem-of-a-2005-flickr-outage-modernized-for-today/"><meta property="og:image" content="https://dathan.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-26T15:38:31-07:00"><meta property="article:modified_time" content="2023-04-26T15:38:31-07:00"><meta property="og:site_name" content="Engineering Thoughts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dathan.github.io/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Postmortem of a 2005 Flickr Outage Modernized for Today"><meta name=twitter:description content="Review of Flickr Architecture Before we delve into the Postmortem let&rsquo;s review the details of the database architecture and Flickr&rsquo;s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it&rsquo;s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dathan.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Postmortem of a 2005 Flickr Outage Modernized for Today","item":"https://dathan.github.io/blog/posts/postmortem-of-a-2005-flickr-outage-modernized-for-today/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Postmortem of a 2005 Flickr Outage Modernized for Today","name":"Postmortem of a 2005 Flickr Outage Modernized for Today","description":"Review of Flickr Architecture Before we delve into the Postmortem let\u0026rsquo;s review the details of the database architecture and Flickr\u0026rsquo;s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it\u0026rsquo;s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years.","keywords":[],"articleBody":"Review of Flickr Architecture Before we delve into the Postmortem let’s review the details of the database architecture and Flickr’s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it’s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years. During these crucial 2 years, I was the main database architect and wrote a bunch of the PHP code that enabled Flickr to reach this level.\nThe next two images from https://twitter.com/iamcal show the overall architecture of Flickr at that time\nDuring my time the node service was not that big of a thing\nWhat is missing from this image is the cache layer for application data, it is combined with the DB layer just to be clear.\nNow, let’s focus on what made the dynamic data respond so quickly for Flickr while handling all the load for millions of Daily Active Users, and typically being a destination for many viral-content endpoints from other social applications. One of the biggest events was in 2008 when Barak Obama won the USA Presidency, and all of his campaign photos including victory night were on Flickr. The Backend Held, the databases held, and Flickr served the content. I was exceptionally proud of Flickr during this time and the work I did for them. It was a great experience working for that Team.\nHistory The database design, setup, offline tasks, Memcache, the PHP code to render the activity feed, Sets, and Collections, along with Stats was my domain. Before we got to the level of handling all that traffic that Flickr was known for, the architecture was a single Main Database (huge) and a few followers that had constant follower lag. Note back in my day, we use to call this Master and Slave Topology but that is wrong for today, so I will focus on the terms of today.\nI have been thinking since 1999 about how one solves a high write throughput while servicing huge amounts of reads. The Main server that handles the writes, with a bunch of read-only followers was a common Web approach of the time. It is perfect for when 90% of the traffic was read-only. Very easy to scale, but this was not Flickr. The ratio for Flickr was more than 40% writes and 60% reads.\nThe writes came from Photo uploads, tagging, 3rd party API access, and every single Photo View produced an update to the database photo view count realtime to show how many times that photo was viewed, and if the stream was also looked at a write realtime to show how many times that Photo Stream was viewed. Additionally, each Photo had a bunch of Machiene tags, and or tags so that we could search for photos. On top of that, each photo also has comments associated with it. Lots of writing activities.\nHow did we handle this?\nDesign to Attain the Goal to handle all the requirements of the product Problem Write intensive Solution More than one written source Problem More than one written source, means multiple single points of Failure Solution Make the write source redundant Problem We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime Solution Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down Problem Everything has to be fast, if a page is slow, Flickr’s customers will be unhappy. Solution Size the databases, to keep most of the data hot so queries that are all indexed can return data quickly Problem Replication Lag, gives a bad experience, yet lag will happen because Solution Let’s tie users to a server for both reads and writes in this design and eliminate the need to see replication. The only time a user could be exposed to replication lag is if the user’s server for the operation is down. We will go into detail about each of the decisions and trade-offs in another section\nThe environment of the time We have a general approach, to the design. Spread data out, we will detail how that is spread out at a later time, but to meet some of our solutions we need to have a server profile that will handle the load.\nThe great side effect of this approach is any server that can run MySQL, regardless of memory or IOPS throughput, can be used. For simplicity’s sake, and to set ourselves up with success, we have a single homogenous server environment for these MySQL servers to make the problem easier to scale and plan when we need new servers.\nThe Hardware was\n4u 2 power supplies connected to 2 different power towers per rack 16GB of RAM 4 CPU per server with 6 15K RPM RAID-10 drives hardware raid with a Battery Backed Cache, the controller was an LSI with 128MB of memory to save data on a power outage. and a 256K STRIPE. RAID-10 (also known as RAID 1+0) is a type of RAID configuration that combines both RAID 1 (mirroring) and RAID 0 (striping) techniques. In a RAID-10 configuration, data is first mirrored across two sets of disks and then the resulting mirrored pairs are striped together.\nFor example, in a 6-disk RAID-10 array, the disks are divided into three mirrored pairs. Data is written to one of the mirrored pairs and then mirrored to the other disk in the same pair. Then the resulting pairs are striped together for improved performance. The stripe size of a RAID-10 array refers to the size of the data blocks that are striped across the disks. The stripe size can affect the performance of the array, as well as the amount of wasted space due to partial stripe writes.\nThis RAID setup is a very important concept because essentially our RAID-10 disk setup conceptually is how our Shard Design is implemented. 2 Servers in a Main-Main config, where writes happen to one server and is mirrored to the other.\n!(sharding)[/blog/img/Federation.png]\nThe MySQL setup at the time\nmysql-4.1\nProblem / Solutions Details Problem Write intensive Solution More than one written source Imagine a database system where we are conceptually making a RAID-10 array out of database servers, but the database conceptually has all the data for a table on a single server originally. In a raid-10 system, there is no guarantee that all data for say the user’s table will be on a single server pair.\nIn the image above we can see that user 1 is allocated to a single shard-pair denoted as 1 server, the same with user 2, user 3, and user 4. This means that if I want their data, I need to query their server. So how do I know where their server is for Flickr?\nWe have a concept known as the Global Ring. The only thing that goes here, are Big-O(1) queries to tell the application where a user is located. Note in later designs I take ranges and store that in configs for the application to remove a lookup to Memcache and or the server itself - but this is how we did it at Flickr.\nIn this GLOBAL RING (the original Main server where data was moved out of) we supported the Lookup of OWNERID -\u003e SHARDID PHOTOID -\u003e OWNERID GROUPID -\u003e SHARDID\nProblem We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime Solution Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down Problem We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime Solution Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down We did all of the database shard logic in PHP, the PHP code of the time was rather awesome and clean, I learned a lot about what it takes to be a software engineer at Flickr.\nWhen going to Flickr the user is cookied and a time is stored in that cookie, for anonymous users. That time was essentially the user_id. This is important because to get around Replication Lag, we needed a consistent experience for both logged-out and logged-in users. They always needed to hit the same server side when viewing other people’s photos (or their photos if logged in) or photo streams. We introduced a concept of a bucket. The bucket code was user_id % 2 and that was the side A or B that you would always hit unless that server on the side was down. This small concept also enabled us to bring down a side and perform maintenance like database alters, because back then we did not have online DDLs. This also means that a single server MUST be sized correctly to handle the load for that population of users if a server in a shard went down. This is also why we did not use a replication ring because if we had 3 servers, they can only handle 33% of the load, and require manual intervention to fix replication.\nFinally, both servers needed to go down for users to experience an outage.\nThis bucket concept also fixes Replication Lag. Replication of the time only had 1 IO thread and 1 SQL thread, meaning 1 thread downloads the binlog, and 1 thread applies the changes. If the server starts lagging too much, this was a leading indicator to rebalance the server.\nProblem Everything has to be fast, if a page is slow, Flickr’s customers will be unhappy. Solution Size the databases, to keep most of the data hot so queries that are all indexed can return data quickly Roughly 400K users were stored on each Shard. If we had an influencer we would size that shard smaller, to a point where only 1 user could be on that shard. Flickr did not have an activity feed of your friends at the time, so we didn’t need to create activity feeds where there was a concept of popular users and the rest, such that forcing the architecture to write a user’s activity to their friends feeds, and popular users wrote their activity to every shard for feed generation.\nFinally, we added a cache layer using Memcache to cache individual photo records and the 1st couple of pages of the photo stream. We would not hit the cache if the owner of the photo was looking at their photos, because we wanted to show the real-time number of views for that photo.\n2005 Flickr outage affecting 1/16 of the user population of Flickr directly and everyone else indirectly Name(s): Dathan Vance Pattishall\nDate: Happened in 2005, modernized for 04/26/2023\nLast modified: 04/26/2023\nSummary In Late 2005 or early 2006, the front ends were blocked on the backend, threads maxed out and our web servers became unresponsive causing a user-visible outage. The static farm was unaffected, and if photos were linked on other websites, the loaded find. Both 3rd party apps, and the Flickr apps, are also affected seeing an outage.\nNormally in the summary, I would put the mitigation but I am going to wait until the end.\nImpact 1/16 of the population was not able to load their streams or upload photos Apache threads built up not servicing requests Uploads did not process for some users an entire shard was offline due to blocked queries 3rd party applications went offline Timeline 6 am 2005 - Pager goes off, offline queue is growing 6:05 am 2005 - An engineer responds to the page and indicates that Apache threads are maxed out 6:15 am 2005 - Shard-3a is holding 3000 threads open, all blocked 6:15 am 2005 - An engineer restarts all Apache servers 6:17 am 2005 - An engineer kills all queries on Shard-3a 6:30 am 2005 - An engineer removes Shard-3a from the pool 6:45 am 2005 - Alerts go off as Shard-3b is slowing down requiring selects to be killed 7:00 am 2005 - Apache threads are higher than normal, the system is back online for all users. 9:00 am 2005 - Shard-3a tests all pass and the server is put back in 10:30 am 2005 - Shard-3a exhibits the same behavior, the site is suggested, but some uploads fail 10:30 am 2005 - Shard-3a is also lagging from Shard-3b events 10:35 am 2005 - Shard-3a is removed from the config, Shard-3b becomes overwhelmed 10:40 am 2005 - An engineer locks all of shard-3 users and marks them for migration taking 1/16 of the population offline 10:45 am 2005 - An engineer starts the migration process of shard-3 users to the rest of the pool 9 pm 2005 - The migration finishes enabling all users to use the site Root Cause(s) Why did the Apache thread pile up?\nPHP makes an open query close connection on every page load. The MySQL server blocked, and had a setting of 10000 connections, that held open a large percentage of Apache threads Why did MySQL lock?\nInnoDB indicates that threads were blocked on the Disk Why did the Disk block the threads?\nAn engineer noticed that the CPU block queue skyrockets periodically on the server and IOPS went to 0 for about 30 seconds Why did all the CPU’s block queue skyrocket periodically?\nAn engineer verified my.cnf settings An engineer verified O_DIRECT settings An engineer verified disk scheduler settings of DEADLINE An engineer verified nice settings An engineer used an LSI tool to look at the state of the Array all is ok and the health of the controller is marked fine Why did the raid array report that the array is healthy if INNODB is blocking the disk subsystem?\nAn engineer searched for the LSI firmware version and looked in a different section of LSI, and found that the battery is not responding An engineer looked at the change log for that firmware and found that the LSI controller will block all IOPS to protect the data saved if the battery does not respond to the probe An engineer realizes that this server has been in production without incident for 2 years nearly, and the battery is no longer healthy The root causes are a few things:\nThe database connection limit is too high for the number of Apache threads to service traffic. The battery probe times out in 30 seconds taking the array offline for that period for this special case and firmware version of the LSI controller. There was no visibility into the health of the controller from our monitoring system Action Items monitor the controller and the raid array reduce the number of max connections replace all old servers batteries flash the firmware to not block but warn about battery failures. ","wordCount":"2536","inLanguage":"en","datePublished":"2023-04-26T15:38:31-07:00","dateModified":"2023-04-26T15:38:31-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dathan.github.io/blog/posts/postmortem-of-a-2005-flickr-outage-modernized-for-today/"},"publisher":{"@type":"Organization","name":"Engineering Thoughts","logo":{"@type":"ImageObject","url":"https://dathan.github.io/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dathan.github.io/blog/ accesskey=h title="Home (Alt + H)"><img src=https://dathan.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dathan.github.io/blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://dathan.github.io/blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dathan.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://dathan.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Postmortem of a 2005 Flickr Outage Modernized for Today</h1><div class=post-meta><span title='2023-04-26 15:38:31 -0700 -0700'>April 26, 2023</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2536 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/dathan/blog/tree/main/content/posts/postmortem-of-a-2005-flickr-outage-modernized-for-today.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=review-of-flickr-architecture>Review of Flickr Architecture<a hidden class=anchor aria-hidden=true href=#review-of-flickr-architecture>#</a></h1><p>Before we delve into the Postmortem let&rsquo;s review the details of the database architecture and Flickr&rsquo;s overall Architecture at that time. Flickr at that time was a PHP shop, running PHP 4, using Smarty as its template Engine in Apache running MODPHP. PHP was everyplace because it&rsquo;s a great engine. I am sure that many hate this statement, that PHP is X or PHP is Y but in reality, it enabled Flickr to jump from a top 100 destination in the USA to a top 5 destination in less than 2 years. During these crucial 2 years, I was the main database architect and wrote a bunch of the PHP code that enabled Flickr to reach this level.</p><p>The next two images from <a href=@iamcal>https://twitter.com/iamcal</a> show the overall architecture of Flickr at that time</p><p><img loading=lazy src=/blog/img/Flickr-Logical-Arch-2007.png alt=flickrlogical></p><p><code>During my time the node service was not that big of a thing</code></p><p><img loading=lazy src=/blog/img/Flickr-Physical-Architecture.png alt=flickrphysical></p><p><code>What is missing from this image is the cache layer for application data, it is combined with the DB layer just to be clear.</code></p><p>Now, let&rsquo;s focus on what made the dynamic data respond so quickly for Flickr while handling all the load for millions of Daily Active Users, and typically being a destination for many viral-content endpoints from other social applications. One of the biggest events was in 2008 when Barak Obama won the USA Presidency, and all of his campaign photos including victory night were on Flickr. The Backend Held, the databases held, and Flickr served the content. I was exceptionally proud of Flickr during this time and the work I did for them. It was a great experience working for that Team.</p><h2 id=history>History<a hidden class=anchor aria-hidden=true href=#history>#</a></h2><p>The database design, setup, offline tasks, Memcache, the PHP code to render the activity feed, Sets, and Collections, along with Stats was my domain. Before we got to the level of handling all that traffic that Flickr was known for, the architecture was a single Main Database (huge) and a few followers that had constant follower lag. Note back in my day, we use to call this Master and Slave Topology but that is wrong for today, so I will focus on the terms of today.</p><p>I have been thinking since 1999 about how one solves a high write throughput while servicing huge amounts of reads. The Main server that handles the writes, with a bunch of read-only followers was a common Web approach of the time. It is perfect for when 90% of the traffic was read-only. Very easy to scale, but this was not Flickr. The ratio for Flickr was more than 40% writes and 60% reads.</p><p>The writes came from Photo uploads, tagging, 3rd party API access, and every single Photo View produced an update to the database photo view count realtime to show how many times that photo was viewed, and if the stream was also looked at a write realtime to show how many times that Photo Stream was viewed. Additionally, each Photo had a bunch of Machiene tags, and or tags so that we could search for photos. On top of that, each photo also has comments associated with it. Lots of writing activities.</p><p>How did we handle this?</p><h3 id=design-to-attain-the-goal-to-handle-all-the-requirements-of-the-product>Design to Attain the Goal to handle all the requirements of the product<a hidden class=anchor aria-hidden=true href=#design-to-attain-the-goal-to-handle-all-the-requirements-of-the-product>#</a></h3><ul><li><code>Problem</code> Write intensive</li><li><code>Solution</code> More than one written source</li><li><code>Problem</code> More than one written source, means multiple single points of Failure</li><li><code>Solution</code> Make the write source redundant</li><li><code>Problem</code> We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime</li><li><code>Solution</code> Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down</li><li><code>Problem</code> Everything has to be fast, if a page is slow, Flickr&rsquo;s customers will be unhappy.</li><li><code>Solution</code> Size the databases, to keep most of the data hot so queries that are all indexed can return data quickly</li><li><code>Problem</code> Replication Lag, gives a bad experience, yet lag will happen because</li><li><code>Solution</code> Let&rsquo;s tie users to a server for both reads and writes in this design and eliminate the need to see replication. The only time a user could be exposed to replication lag is if the user&rsquo;s server for the operation is down.</li></ul><p>We will go into detail about each of the decisions and trade-offs in another section</p><h3 id=the-environment-of-the-time>The environment of the time<a hidden class=anchor aria-hidden=true href=#the-environment-of-the-time>#</a></h3><p>We have a general approach, to the design. Spread data out, we will detail how that is spread out at a later time, but to meet some of our solutions we need to have a server profile that will handle the load.</p><p>The great side effect of this approach is any server that can run MySQL, regardless of memory or IOPS throughput, can be used. For simplicity&rsquo;s sake, and to set ourselves up with success, we have a single homogenous server environment for these MySQL servers to make the problem easier to scale and plan when we need new servers.</p><p>The Hardware was</p><ul><li>4u</li><li>2 power supplies connected to 2 different power towers per rack</li><li>16GB of RAM</li><li>4 CPU per server</li><li>with 6 15K RPM RAID-10 drives</li><li>hardware raid with a Battery Backed Cache, the controller was an LSI with 128MB of memory to save data on a power outage.</li><li>and a 256K STRIPE.</li></ul><p>RAID-10 (also known as RAID 1+0) is a type of RAID configuration that combines both RAID 1 (mirroring) and RAID 0 (striping) techniques. In a RAID-10 configuration, data is first mirrored across two sets of disks and then the resulting mirrored pairs are striped together.</p><p>For example, in a 6-disk RAID-10 array, the disks are divided into three mirrored pairs. Data is written to one of the mirrored pairs and then mirrored to the other disk in the same pair. Then the resulting pairs are striped together for improved performance. The stripe size of a RAID-10 array refers to the size of the data blocks that are striped across the disks. The stripe size can affect the performance of the array, as well as the amount of wasted space due to partial stripe writes.</p><p><code>This RAID setup is a very important concept because essentially our RAID-10 disk setup conceptually is how our Shard Design is implemented. 2 Servers in a Main-Main config, where writes happen to one server and is mirrored to the other.</code></p><p>!(sharding)[/blog/img/Federation.png]</p><p>The MySQL setup at the time</p><p><code>mysql-4.1</code></p><h2 id=problem--solutions-details>Problem / Solutions Details<a hidden class=anchor aria-hidden=true href=#problem--solutions-details>#</a></h2><blockquote><ul><li><code>Problem</code> Write intensive</li><li><code>Solution</code> More than one written source</li></ul></blockquote><p>Imagine a database system where we are conceptually making a RAID-10 array out of database servers, but the database conceptually has all the data for a table on a single server originally. In a raid-10 system, there is no guarantee that all data for say the user&rsquo;s table will be on a single server pair.</p><p>In the image above we can see that user 1 is allocated to a single shard-pair denoted as 1 server, the same with user 2, user 3, and user 4. This means that if I want their data, I need to query their server. So how do I know where their server is for Flickr?</p><p>We have a concept known as the Global Ring. The only thing that goes here, are Big-O(1) queries to tell the application where a user is located. <em>Note in later designs I take ranges and store that in configs for the application to remove a lookup to Memcache and or the server itself - but this is how we did it at Flickr.</em></p><p>In this GLOBAL RING (the original Main server where data was moved out of) we supported the Lookup of
OWNERID -> SHARDID
PHOTOID -> OWNERID
GROUPID -> SHARDID</p><blockquote><ul><li><code>Problem</code> We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime</li><li><code>Solution</code> Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down</li><li><code>Problem</code> We want to make frequent changes to the schema, upgrade, and or allocate servers without a downtime</li><li><code>Solution</code> Stick users (anonymous and logged in) to a server AND that single server needs to handle all the reads and writes if its pair went down</li></ul></blockquote><p>We did all of the database shard logic in PHP, the PHP code of the time was rather awesome and clean, I learned a lot about what it takes to be a software engineer at Flickr.</p><p>When going to Flickr the user is cookied and a time is stored in that cookie, for anonymous users. That time was essentially the user_id. This is important because to get around Replication Lag, we needed a consistent experience for both logged-out and logged-in users. They always needed to hit the same server <em>side</em> when viewing other people&rsquo;s photos (or their photos if logged in) or photo streams. We introduced a concept of a bucket. The bucket code was <code>user_id % 2</code> and that was the side A or B that you would always hit unless that server on the side was down. This small concept also enabled us to bring down a side and perform maintenance like database alters, because back then we did not have online DDLs. This also means that a single server MUST be sized correctly to handle the load for that population of users if a server in a shard went down. This is also why we did not use a replication ring because if we had 3 servers, they can only handle 33% of the load, and require manual intervention to fix replication.</p><p>Finally, both servers needed to go down for users to experience an outage.</p><p>This bucket concept also fixes Replication Lag. Replication of the time only had 1 IO thread and 1 SQL thread, meaning 1 thread downloads the binlog, and 1 thread applies the changes. If the server starts lagging too much, this was a leading indicator to rebalance the server.</p><blockquote><ul><li><code>Problem</code> Everything has to be fast, if a page is slow, Flickr&rsquo;s customers will be unhappy.</li><li><code>Solution</code> Size the databases, to keep most of the data hot so queries that are all indexed can return data quickly</li></ul></blockquote><p>Roughly 400K users were stored on each Shard. If we had an influencer we would size that shard smaller, to a point where only 1 user could be on that shard. Flickr did not have an activity feed of your friends at the time, so we didn&rsquo;t need to create activity feeds where there was a concept of popular users and the rest, such that forcing the architecture to write a user&rsquo;s activity to their friends feeds, and popular users wrote their activity to every shard for feed generation.</p><p>Finally, we added a cache layer using Memcache to cache individual photo records and the 1st couple of pages of the photo stream. We would not hit the cache if the owner of the photo was looking at their photos, because we wanted to show the real-time number of views for that photo.</p><h1 id=2005-flickr-outage-affecting-116-of-the-user-population-of-flickr-directly-and-everyone-else-indirectly>2005 Flickr outage affecting 1/16 of the user population of Flickr directly and everyone else indirectly<a hidden class=anchor aria-hidden=true href=#2005-flickr-outage-affecting-116-of-the-user-population-of-flickr-directly-and-everyone-else-indirectly>#</a></h1><p>Name(s): Dathan Vance Pattishall</p><p>Date: Happened in 2005, modernized for 04/26/2023</p><p>Last modified: 04/26/2023</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>In Late 2005 or early 2006, the front ends were blocked on the backend, threads maxed out and our web servers became unresponsive causing a user-visible outage. The static farm was unaffected, and if photos were linked on other websites, the loaded find. Both 3rd party apps, and the Flickr apps, are also affected seeing an outage.</p><p>Normally in the summary, I would put the mitigation but I am going to wait until the end.</p><h2 id=impact>Impact<a hidden class=anchor aria-hidden=true href=#impact>#</a></h2><ul><li>1/16 of the population was not able to load their streams or upload photos</li><li>Apache threads built up not servicing requests</li><li>Uploads did not process for some users</li><li>an entire shard was offline due to blocked queries</li><li>3rd party applications went offline</li></ul><h2 id=timeline>Timeline<a hidden class=anchor aria-hidden=true href=#timeline>#</a></h2><ul><li>6 am 2005 - Pager goes off, offline queue is growing</li><li>6:05 am 2005 - An engineer responds to the page and indicates that Apache threads are maxed out</li><li>6:15 am 2005 - Shard-3a is holding 3000 threads open, all blocked</li><li>6:15 am 2005 - An engineer restarts all Apache servers</li><li>6:17 am 2005 - An engineer kills all queries on Shard-3a</li><li>6:30 am 2005 - An engineer removes Shard-3a from the pool</li><li>6:45 am 2005 - Alerts go off as Shard-3b is slowing down requiring selects to be killed</li><li>7:00 am 2005 - Apache threads are higher than normal, the system is back online for all users.</li><li>9:00 am 2005 - Shard-3a tests all pass and the server is put back in</li><li>10:30 am 2005 - Shard-3a exhibits the same behavior, the site is suggested, but some uploads fail</li><li>10:30 am 2005 - Shard-3a is also lagging from Shard-3b events</li><li>10:35 am 2005 - Shard-3a is removed from the config, Shard-3b becomes overwhelmed</li><li>10:40 am 2005 - An engineer locks all of shard-3 users and marks them for migration taking 1/16 of the population offline</li><li>10:45 am 2005 - An engineer starts the migration process of shard-3 users to the rest of the pool</li><li>9 pm 2005 - The migration finishes enabling all users to use the site</li></ul><h2 id=root-causes>Root Cause(s)<a hidden class=anchor aria-hidden=true href=#root-causes>#</a></h2><p>Why did the Apache thread pile up?</p><ul><li>PHP makes an open query close connection on every page load. The MySQL server blocked, and had a setting of 10000 connections, that held open a large percentage of Apache threads</li></ul><p>Why did MySQL lock?</p><ul><li>InnoDB indicates that threads were blocked on the Disk</li></ul><p>Why did the Disk block the threads?</p><ul><li>An engineer noticed that the CPU block queue skyrockets periodically on the server and IOPS went to 0 for about 30 seconds</li></ul><p>Why did all the CPU&rsquo;s block queue skyrocket periodically?</p><ul><li>An engineer verified my.cnf settings</li><li>An engineer verified O_DIRECT settings</li><li>An engineer verified disk scheduler settings of DEADLINE</li><li>An engineer verified nice settings</li><li>An engineer used an LSI tool to look at the state of the Array all is ok and the health of the controller is marked fine</li></ul><p>Why did the raid array report that the array is healthy if INNODB is blocking the disk subsystem?</p><ul><li>An engineer searched for the LSI firmware version and looked in a different section of LSI, and found that the battery is not responding</li><li>An engineer looked at the change log for that firmware and found that the LSI controller will block all IOPS to protect the data saved if the battery does not respond to the probe</li><li>An engineer realizes that this server has been in production without incident for 2 years nearly, and the battery is no longer healthy</li></ul><p>The root causes are a few things:</p><ul><li>The database connection limit is too high for the number of Apache threads to service traffic.</li><li>The battery probe times out in 30 seconds taking the array offline for that period for this special case and firmware version of the LSI controller.</li><li>There was no visibility into the health of the controller from our monitoring system</li></ul><h2 id=action-items>Action Items<a hidden class=anchor aria-hidden=true href=#action-items>#</a></h2><ul><li>monitor the controller and the raid array</li><li>reduce the number of max connections</li><li>replace all old servers batteries</li><li>flash the firmware to not block but warn about battery failures.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://dathan.github.io/blog/posts/using-playwright-with-golang-and-python-to-grab-js-generated-csv/><span class=title>Next »</span><br><span>Using Playwright With Golang and Python to Grab Js Generated Csv</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on twitter" href="https://twitter.com/intent/tweet/?text=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today&url=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f&title=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today&summary=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today&source=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f&title=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on whatsapp" href="https://api.whatsapp.com/send?text=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today%20-%20https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Postmortem of a 2005 Flickr Outage Modernized for Today on telegram" href="https://telegram.me/share/url?text=Postmortem%20of%20a%202005%20Flickr%20Outage%20Modernized%20for%20Today&url=https%3a%2f%2fdathan.github.io%2fblog%2fposts%2fpostmortem-of-a-2005-flickr-outage-modernized-for-today%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dathan.github.io/blog/>Engineering Thoughts</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>